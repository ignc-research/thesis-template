% Spellchecking done with grammarly
\chapter{Related Works (BA: 2-3 pages, MA: 3-5 pages)}
The development of this work occurs in arena-rosnav \citep{rosnav} a 2D simulator with a DRL training and testing pipeline for training agents to navigate in collaborative environments with dynamic obstacles. In this framework, \citep{kastner} DRL training for autonomous navigation in collaborative environments was implemented and integrated into ROS. Low collision rates were achieved with DRL agents, even in environments with 20 dynamic obstacles. However, there are limitations with long training times and high computational resource requirements: training can run for several days when done with NVIDIA RTX 3090 GPU and 25 CPU-cores.
\\\\\noindent 
Imitation learning has the potential to accelerate this process by producing a good set of weights that are better than those of a randomly initialized agent. A precedent was set by Pfeiffer et al.: in two papers, they set out a pipeline for accelerating DRL with behavior cloning (BC). BC is a form of IL in which an agent is trained to imitate an expert by performing supervised learning on observations from the agent’s environment, labelled with an expert’s chosen action \citep{abbeel}.
\\\\\noindent 
In the first paper, only behavior cloning was used to train an agent to navigate in an environment without a map \citep{pfeiffer1}. They recorded 6,000 demonstrations using the DWA planner with a total of 2.1 million timesteps. Their BC experiments produced an agent that was able navigate with a performance comparable to DWA in a previously unseen map. Importantly, the training and test environments did not include dynamic obstacles.
\\\\\noindent 
In the second paper, Pfeiffer et al. used behavior cloning to accelerate DRL training \citep{pfeiffer2}. The agents were pretrained using the procedure above, again without dynamic obstacles. Agents were either pretrained with 10 demonstrations, pretrained with 1,000 demonstrations or not pretrained at all. During DRL, the success rate and collision rate across training iterations were compared. It was found that the DRL training time was reduced by 80\% by using behavior cloning on 1,000 demonstrations, reaching a comparable level of performance in success rate and collision rate after 200 DRL iterations that required 1,000 iterations for pure DRL. The authors hypothesize that behavior cloning enables the agent to explore more efficiently, reducing the training time.
\\\\\noindent 
One of the contributions of this project is to implement the data generation and behavior cloning functionality necessary to do similar experiments. Demonstrations recorded with it could be reused to reduce training time across different DRL training runs in the arena-rosnav project. Additionally, this project investigated training agents to avoid random dynamic obstacles as well, which were not considered by \citep{pfeiffer1, pfeiffer2}. Furthermore, the implementation allows demonstrations to be recorded by a human expert, which can be used to train specialized behaviors.
\\\\\noindent 
Behavior cloning is not limited to robotic navigation. An example includes training an agent to play Counter-Strike: Global Offensive using only expert demonstrations \citep{counterstrike}.
Imitation learning was also used by DeepMind to train AlphaGo to play Go: it was first trained on expert moves before continuing training by trial and error with DRL \citep{silver}.
\\\\\noindent 
Likewise, behavior cloning can be used to train specific tasks. For example, Zhang et al. trained a neural network to control a robotic arm to perform tasks such as pushing a LEGO block into a target zone and grasping objects \citep{zhang}. These were learned from expert demonstrations which were recorded by letting a human teleoperate the arm. To achieve success rates close to 100\%, expert datasets equivalent to 11.1 minutes were required for grasping and 16.9 minutes for pushing.
Another example is handwriting recognition, which was learned using DAGGER in \citep{dagger}.
