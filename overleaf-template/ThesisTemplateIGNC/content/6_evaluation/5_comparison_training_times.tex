\section{Comparison of DRL Training Times}
Of the two experiments, training an agent using only DRL and training an agent using behavior cloning and DRL, only pure DRL was successful. The training script converged after 3.4 days and the agent exhibits excellent navigation and obstacle avoidance performance, even in the presence of 10 dynamic obstacles. In comparison, the training script did not converge when starting with the agent pretrained on human expert data, instead timing out after five days of training. The performance of this agent was excellent in the absence of dynamic obstacles, but poor in their presence, showing that it did not learn to navigate in collaborative environments. Therefore, behavior cloning did not accelerate DRL in collaborative environments.

