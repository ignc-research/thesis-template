\section{Proximal Policy Optimization}
The DRL algorithm used in arena-rosnav is Proximal Policy Optimization (PPO). For this paper, the key aspect of PPO is that it is an on-policy algorithm \citep{ppo}. On-policy means that the network being trained is the same one that is used to gather training data in the form of episodes in the environment \citep{lapan}. As a result, after each training iteration, the episodes must be discarded and fresh training data must be collected. This is because the old training data was generated by a policy that is, generally, worse than the one currently being trained \citep{lapan}.
This leads to a high sample inefficiency. A possible solution could be first training on expert demonstrations, which are episodes that show the targeted behavior of the agent. This area of research is Imitation Learning (IL) and is the topic of the following section.
