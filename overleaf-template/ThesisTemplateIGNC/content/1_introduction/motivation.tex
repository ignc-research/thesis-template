\section{Motivation}
Autonomous robotic navigation is the problem of creating robots that can independently plan and execute a path from a starting position to a goal. In collaborative environments, space is also shared with other entities, such as other robots, humans, or forklifts, so behavior must be adjusted dynamically. These decisions are based on sensor data, for example laser scans and RGB-cameras, and can be further supported over a network, for example GPS location, or the location of other humans and robots in a facility.
\\\\\noindent 
Technology that enables autonomous robotic navigation is important for society. It is the fundamental technology underpinning self-driving cars, which have the potential to revolutionize the transportation industry and how cities are structured. Self-driving robots are also increasingly investigated for use in industrial settings, such as smart factories. They are also finding important applications in environments which are too dangerous for humans, for example search-and-rescue missions and the NASA Perseverance rover.
\\\\\noindent 
Deep Reinforcement Learning (DRL) has emerged as a promising approach to autonomous navigation. In reinforcement learning, an agent learns by trial-and-error, usually in a simulated environment, and receives feedback on its behavior from a reward function. This has led to state-of-the-art performance, however there are problems with this approach, like long training times and high computational resource requirements.
\\\\\noindent 
One approach that has emerged to address these issues is Imitation Learning (IL). It encompasses a variety of techniques which all share the objective of training an agent to reproduce the behavior demonstrated by an expert \citep{abbeel}. This expert can be any entity that can exhibit the targeted behavior, such as a human, a traditional motion planner like MPC or another DRL-trained agent. It can be used to replace DRL-training altogether or to accelerate it. Expert demonstrations can also be used to teach an agent specific behaviors for situations that are unlikely to come up in a randomized simulation environment, or that are not feasible to encode in a reward function.
